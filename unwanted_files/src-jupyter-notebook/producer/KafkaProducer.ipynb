{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-79abede6b271>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mitertools\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mlogging\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;31m#from kafka import KafkaProducer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mhelper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import pandas as pd\n",
    "#from kafka import KafkaProducer\n",
    "import helper\n",
    "import helper2\n",
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import jsonimport itertools\n",
    "import logging\n",
    "import pandas as pd\n",
    "#from kafka import KafkaProducer\n",
    "import helper\n",
    "import helper2\n",
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T19:40:41.338523Z",
     "start_time": "2024-01-19T19:40:41.326519Z"
    }
   },
   "id": "1359fe30e7b0ab8f",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'IncompatibleBrokerVersion' from 'kafka.errors' (/opt/homebrew/lib/python3.11/site-packages/kafka/errors.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkafka\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KafkaProducer\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/kafka/__init__.py:21\u001B[0m\n\u001B[1;32m     16\u001B[0m             \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m     18\u001B[0m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\u001B[38;5;241m.\u001B[39maddHandler(NullHandler())\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkafka\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madmin\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KafkaAdminClient\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkafka\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclient_async\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KafkaClient\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkafka\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconsumer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KafkaConsumer\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/kafka/admin/__init__.py:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m absolute_import\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkafka\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madmin\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig_resource\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConfigResource, ConfigResourceType\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkafka\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madmin\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclient\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KafkaAdminClient\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkafka\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madmin\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01macl_resource\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (ACL, ACLFilter, ResourcePattern, ResourcePatternFilter, ACLOperation,\n\u001B[1;32m      6\u001B[0m                                       ResourceType, ACLPermissionType, ACLResourcePatternType)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkafka\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madmin\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnew_topic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NewTopic\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.11/site-packages/kafka/admin/client.py:16\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkafka\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcoordinator\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConsumerProtocolMemberMetadata, ConsumerProtocolMemberAssignment, ConsumerProtocol\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkafka\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mErrors\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkafka\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     17\u001B[0m     IncompatibleBrokerVersion, KafkaConfigurationError, NotControllerError,\n\u001B[1;32m     18\u001B[0m     UnrecognizedBrokerVersion, IllegalArgumentError)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkafka\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MetricConfig, Metrics\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkafka\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madmin\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     21\u001B[0m     CreateTopicsRequest, DeleteTopicsRequest, DescribeConfigsRequest, AlterConfigsRequest, CreatePartitionsRequest,\n\u001B[1;32m     22\u001B[0m     ListGroupsRequest, DescribeGroupsRequest, DescribeAclsRequest, CreateAclsRequest, DeleteAclsRequest,\n\u001B[1;32m     23\u001B[0m     DeleteGroupsRequest\n\u001B[1;32m     24\u001B[0m )\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'IncompatibleBrokerVersion' from 'kafka.errors' (/opt/homebrew/lib/python3.11/site-packages/kafka/errors.py)"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T19:29:40.767653Z",
     "start_time": "2024-01-19T19:29:40.712881Z"
    }
   },
   "id": "d6b6377bab434ab",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: kafka-python 2.0.2\r\n",
      "Uninstalling kafka-python-2.0.2:\r\n",
      "  Would remove:\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/*\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka_python-2.0.2.dist-info/*\r\n",
      "  Would not remove (might be manually added):\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/client.py\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/common.py\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/consumer/base.py\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/consumer/multiprocess.py\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/consumer/simple.py\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/context.py\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/partitioner/base.py\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/partitioner/hashed.py\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/partitioner/roundrobin.py\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/producer/base.py\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/producer/keyed.py\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/producer/simple.py\r\n",
      "    /opt/homebrew/lib/python3.11/site-packages/kafka/protocol/legacy.py\r\n",
      "Proceed (Y/n)? "
     ]
    }
   ],
   "source": [
    "! pip uninstall kafka\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-19T19:33:47.878361Z"
    }
   },
   "id": "2e58dd35adf48abd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Producer:\n",
    "\n",
    "    def __init__(self,serveruri) -> None:\n",
    "        self.topic_methods = {\n",
    "            \"seizure\": self.seizure_process,\n",
    "            \"ukrain\": self.ukrain_process,\n",
    "        }\n",
    "\n",
    "        self.hlpr = helper.Tools()\n",
    "        self.serializer = helper.Serializer()\n",
    "        self.producer_bootstrap_servers = json.loads(serveruri[0])\n",
    "        config = self.hlpr.read_config('producer.properties')\n",
    "        producer_config_args = {\n",
    "                'security_protocol': config.get('security.protocol', 'PLAINTEXT'),\n",
    "                'sasl_mechanism': config.get('sasl.mechanism', 'PLAIN'),\n",
    "                'sasl_plain_username': config.get('sasl.jaas.config').split('username=')[1].split(' ')[0].replace(\"\\\"\", \"\").strip(),\n",
    "                'sasl_plain_password': config.get('sasl.jaas.config').split('password=')[1].replace(\"\\\"\", \"\").replace(\";\", \"\").strip(),\n",
    "        }\n",
    "        \n",
    "        additional_args = {'value_serializer': self.serializer.str_serializer, 'acks': 1, 'linger_ms': 100,'compression_type': 'lz4','batch_size': 16384, **producer_config_args}\n",
    "        # ,'max_request_size':320 * 1024,\n",
    "        #        }\n",
    "        # api_version=(2,7,1),\n",
    "        try:\n",
    "            self.producer = KafkaProducer(bootstrap_servers=self.producer_bootstrap_servers, **additional_args)\n",
    "            print(\"Kafka Producer is running\")\n",
    "        except Exception as ex:\n",
    "            #logging.error('Exception while creating Kafka Producer: ' + str(ex))\n",
    "            print('Exception while creating Kafka Producer')\n",
    "            print(str(ex))\n",
    "    \n",
    "    def send_message(self, topic, message):\n",
    "        try:\n",
    "            self.producer.send(topic, value=message)\n",
    "            self.producer.flush()\n",
    "            print(f\"Message successfully sent to topic '{topic}'\")\n",
    "        except Exception as e:\n",
    "            # Log any exceptions that occur during send\n",
    "            print(f\"Error sending message to topic '{topic}': {e}\")\n",
    " \n",
    "\n",
    "    def close(self):\n",
    "\n",
    "        self.producer.close()\n",
    "\n",
    "    def fetch_files(self,pri,pi,input_path):\n",
    "        file_names = []\n",
    "        pattern = f\"{input_path}/*-pod-{pi}-prod-{pri}.csv\"\n",
    "        matched_files = glob.glob(pattern)\n",
    "        file_names.extend(matched_files)\n",
    "        return file_names\n",
    "\n",
    "\n",
    "\n",
    "    def stream_data(self,wait_time,topicTitle,**kwargs):\n",
    "        while True:\n",
    "            filenames = self.fetch_files(kwargs.get('prodindex'),kwargs.get('podindex'),kwargs.get('inputpath'))\n",
    "            if filenames:\n",
    "                process_method = self.topic_methods.get(topicTitle)\n",
    "                if not process_method:\n",
    "                    raise ValueError(f\"No processing method found for topic: {topicTitle}\")\n",
    "                process_method(filenames, topicTitle, **kwargs)\n",
    "            else:\n",
    "                time.sleep(wait_time)  # Sleep for a while before checking again\n",
    "\n",
    "\n",
    "\n",
    "    def ukrain_process(self, filenames, topicTitle, nprod , num_topics, podindex, prodindex, batchsize, column_range, inputpath):\n",
    "\n",
    "        sindex = podindex * batchsize * nprod + prodindex * batchsize\n",
    "        unique_words = set()\n",
    "        for filename in filenames:\n",
    "            print(\"################################################################\")\n",
    "            print(f\" File name is {filename}\")\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                header = f.readline().split(',')\n",
    "            itr = 0\n",
    "            key = 0\n",
    "           # while (True):\n",
    "            print(f'iteration={itr}')\n",
    "            itr = itr + 1\n",
    "            df = pd.read_csv(filename, lineterminator='\\n', names=header, header=0, nrows=batchsize, skiprows=sindex, usecols = header, encoding='utf-8')\n",
    "           # print(\"df is read\")\n",
    "            if df.shape[0] == 0:\n",
    "                print('EOF')\n",
    "                break\n",
    "                #        start_timestamp = df['timestamp'][0]\n",
    "                #        producer.send(\"timestamp\" , start_timestamp)\n",
    "            df['text'] = df['text\\n'].apply(lambda x: x.split(' '))\n",
    "            df['text'] = df['text'].map(lambda x: helper2.cleanHashtags(x))\n",
    "                # df['text'] = df['text'].map(lambda x: list(filter(lambda y: y != 'vacc' and y != 'vaccination', x)))\n",
    "            df = df[df['text'].apply(lambda x: len(x) > 1)]\n",
    "            if df.empty:\n",
    "                print('Empty DF?')\n",
    "            else:\n",
    "\n",
    "                    ## Finding the range of colums for developing CSR matrix\n",
    "                    #col_range = self.hlpr.findColRange(df)\n",
    "                \n",
    "                rangehash = partial(helper2.rangeHash, r = column_range)\n",
    "\n",
    "                print(\"start finding word pairs\")\n",
    "                for i in range(len(df)):\n",
    "                    df['word-pairs_ls'] = list(map(lambda x: list(itertools.permutations(x, r=2)), df['text']))\n",
    "                df_series = df['word-pairs_ls'].explode().apply(pd.Series)\n",
    "                df_series.columns = ['word1', 'word2']\n",
    "                df_series['word1'] = df_series['word1'].map(lambda x: rangehash(x))\n",
    "                df_series['word2'] = df_series['word2'].map(lambda x: rangehash(x))\n",
    "                dfResult = df_series.groupby('word1').agg(list).apply(lambda x: list(zip(*x)), axis=1)\n",
    "                for i in range(dfResult.shape[0]):\n",
    "\n",
    "                        #producerTimestamp = self.hlpr.TimestampEvent()\n",
    "                     print('----------------------------------------------------------------')\n",
    "                     print(\"Producer is Sending Data\")\n",
    "                     hashTopic = dfResult.index[i]\n",
    "                     word = helper2.get_word_from_hash(hashTopic)  # Lookup the word from the hash\n",
    "                     key = hashTopic % num_topics\n",
    "                     topic = f\"{topicTitle}_{key}\"#, {word}\"\n",
    "                     print (f\" Topic is {topic}, {word}\")\n",
    "                     flat_list = [item for sublist in dfResult.iloc[i] for item in sublist]\n",
    "                     #dictkey= f\"{key}, {word}\"                                    #dict_keyi = str(key) + \", \" + word\n",
    "                    # print(f\" type word is {type(word)}\")\n",
    "                    # print(f\" type dictkey is {type(dictkey)}\")\n",
    "                     #valuedict = {dictkey: flat_list}\n",
    "                    # valuedict = str({hashTopic: dfResult.iloc[i]})\n",
    "                     valuedict = str({hashTopic: flat_list})\n",
    "                     print(f\"ValueDict ={valuedict}\")\n",
    "\n",
    "                     # Find the number of unique words in the dictionary including the key\n",
    "                     # Initialize a set with the key\n",
    "                     unique_words.add(hashTopic)\n",
    "                     for value in flat_list:\n",
    "                         unique_words.add(value)\n",
    "                     #words_count = len(words_values)  # the word count in each message\n",
    "                     print(f\"Unique words count until now is {len(unique_words)}\")\n",
    "                     #headers = [('words_count', bytes(str(words_count), encoding ='utf-8'))] ## send the word count as header through producer\n",
    "                     self.send_message(topic, valuedict)\n",
    "                     print('****************************************************************')\n",
    "        words_len = len(unique_words)\n",
    "        print(f\" Number of unique words producer sent through Kafka is : {words_len}\")\n",
    "        print(f\" Unique words producer sent through Kafka are : {unique_words}\")\n",
    "        os.remove(filename)\n",
    "\n",
    "    def seizure_process(self, filenames, topicTitle, _range):\n",
    "\n",
    "        ## Call method for producing fake data and sending it to kafka by producer\n",
    "\n",
    "        for _ in range(_range):\n",
    "            message = self.hlpr.produceFakeData()\n",
    "            self.send_message(topicTitle, message)\n",
    "        print(f\"Done with sending data by producer to topic {topicTitle}\")\n",
    "        self.close()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": 0
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
